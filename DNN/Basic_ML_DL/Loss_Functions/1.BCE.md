# Binary Cross Entropy (Log Loss)

$$
\text{BCE}(y, \hat{y}) = - \left[ y \cdot \log(\hat{y}) + (1 - y) \cdot \log(1 - \hat{y}) \right]
$$


BCE는 확률값을 직접 예측할 때 사용.

$$ 
(\hat{y}) = Sigmoid \ output
$$
정답과 예측 확률 간의 정보의 차이를 측정하는 방식.

로그함수이기 때문에 정답에서 멀어질수록 급격히 증가. 

---

### Additiional

📌 PyTorch: `nn.BCELoss`, `nn.BCEWithLogitsLoss` (더 안정적)
