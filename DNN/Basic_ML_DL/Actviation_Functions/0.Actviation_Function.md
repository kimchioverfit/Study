# Activiation Function

활성함수란?

다른 말로는 비선형 함수

이걸 연산해줘야 비선형성을 통해 선형 연산으로는 구하지 못하는

해들을 구해낼 수 있음. 

딥러닝에서는 활성함수라고 부른다. 

용도에 따라서 다른 활성화 함수를 쓴다. 

### 자주 쓰이는 활성화 함수 

| 함수             | 수식                                  | 특징                                   | 대표 사용처                      |
| -------------- | ----------------------------------- | ------------------------------------ | --------------------------- |
| **ReLU**       | $\max(0, x)$                        | 빠르고 단순. 음수는 0으로                      | 대부분의 **은닉층**                |
| **Leaky ReLU** | $\max(\alpha x, x)$, $\alpha \ll 1$ | 음수도 약간 통과 (죽은 ReLU 방지)               | **ReLU로 gradient가 사라질 때**   |
| **Sigmoid**    | $\frac{1}{1 + e^{-x}}$              | 0\~1 출력, 미분값 작음 (vanishing gradient) | **이진 분류 출력층**               |
| **Tanh**       | $\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | -1\~1 출력, 중심이 0                      | 과거 은닉층 (지금은 ReLU가 더 많이 사용됨) |
| **Softmax**    | $\frac{e^{x_i}}{\sum_j e^{x_j}}$    | 전체 합이 1인 확률 벡터 출력                    | **다중 클래스 분류 출력층**           |
| **Swish**      | $x \cdot \text{sigmoid}(x)$         | 부드럽고 성능이 좋음 (구글 제안)                  | 성능 중요할 때 (→ 효율성 ↔ 정확도)      |
| **GELU**       | $x \cdot \Phi(x)$ (정규 누적분포 기반)      | GPT 계열 모델에서 사용                       | **Transformer 모델 등 고성능 모델** |
