
# Sigmoid 함수에 대한 상세 설명

## 📌 정의 (수학식)

시그모이드 함수는 입력값을 부드럽게 0과 1 사이의 값으로 매핑하는 비선형 함수입니다.

\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]

- 입력 \( x \in \mathbb{R} \)
- 출력 \( \sigma(x) \in (0, 1) \)

---

## 📌 특성

- **S자형 곡선 (S-shaped curve)**
- \( \sigma(0) = 0.5 \)
- \( \lim_{x \to -\infty} \sigma(x) = 0 \), \( \lim_{x \to \infty} \sigma(x) = 1 \)

### 시각적 예시:

```
    1 |                                   ●
      |                              ●●
      |                          ●●
      |                       ●●
      |                    ●●
 0.5 |----------●----------●----------> x
      |     ●●
      |  ●●
      |●
    0 |
```

---

## 📌 미분

시그모이드 함수의 도함수는 다음과 같습니다:

\[
\frac{d\sigma(x)}{dx} = \sigma(x)(1 - \sigma(x))
\]

이 표현은 역전파(Backpropagation) 알고리즘에서 매우 유용하게 사용됩니다.

---

## 📌 장점

- ✅ 출력이 0과 1 사이 → 확률 해석에 적합
- ✅ 미분 가능 → 학습 가능
- ✅ 부드러운 곡선 → 수치적으로 안정적

---

## 📌 단점

- ❌ **Vanishing Gradient 문제**
  - 입력값이 커지면 도함수가 0에 가까워져 학습이 느려짐

- ❌ **출력 중심이 0이 아님**
  - 평균이 0이 아니기 때문에 학습 효율이 떨어질 수 있음

---

## 📌 다른 활성화 함수와 비교

| 함수     | 출력 범위 | 중심  | 특징                              |
|----------|-----------|--------|-----------------------------------|
| Sigmoid  | (0, 1)    | 0.5    | 이진 분류 출력층에서 사용         |
| Tanh     | (-1, 1)   | 0      | Sigmoid보다 학습에 더 적합        |
| ReLU     | [0, ∞)    | 0      | 빠른 수렴, 현대 딥러닝의 표준     |

---

## 📌 시그모이드 함수 시각화 (Python 예제)

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

x = np.linspace(-10, 10, 100)
y = sigmoid(x)

plt.plot(x, y)
plt.title("Sigmoid Function")
plt.grid()
plt.show()
```

---

## 📌 주요 활용처

- 이진 분류 문제의 출력층
- 과거 MLP, RNN 계열의 기본 활성화 함수
- 현재는 주로 출력층에서 사용됨

---
