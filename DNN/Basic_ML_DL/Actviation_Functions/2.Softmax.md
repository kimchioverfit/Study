
# Softmax 함수에 대한 상세 설명

## 📌 정의 (수학식)

Softmax 함수는 벡터 형태의 입력을 받아 각 요소를 0~1 사이의 값으로 변환하고, 전체 합이 1이 되도록 만드는 **확률 분포 함수**입니다.

주로 다중 클래스 분류(Multi-class Classification) 문제의 출력층에서 사용됩니다.

\[
	ext{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
\]

- 입력: 벡터 \( \mathbf{z} = [z_1, z_2, ..., z_K] \)
- 출력: 확률 벡터 \( \mathbf{p} = [p_1, p_2, ..., p_K] \), 단 \( \sum_{i=1}^{K} p_i = 1 \)

---

## 📌 예시

입력 벡터: \( z = [1.0, 2.0, 3.0] \)

```python
import numpy as np

z = np.array([1.0, 2.0, 3.0])
softmax = np.exp(z) / np.sum(np.exp(z))
print(softmax)
```

출력 (확률 분포):  
\( [0.0900, 0.2447, 0.6652] \)

---

## 📌 특성

- 출력값은 모두 양수이며 전체 합은 1 (확률처럼 해석 가능)
- 입력값의 차이가 클수록, 큰 값은 더 커지고 작은 값은 더 작아지는 경향이 있음 (분류를 선명하게 만듦)
- 입력 벡터의 **스케일에 민감** → 큰 값이 있으면 softmax는 해당 클래스에 확률을 거의 몰아줌

---

## 📌 수치 안정성 처리

Softmax는 입력이 클 경우 overflow가 발생할 수 있기 때문에 다음과 같이 **입력 최대값을 빼서 계산**합니다:

\[
	ext{Softmax}(z_i) = \frac{e^{z_i - \max(z)}}{\sum_{j=1}^{K} e^{z_j - \max(z)}}
\]

이렇게 하면 softmax의 값은 같으면서도, 수치적 안정성이 향상됩니다.

---

## 📌 미분 (역전파에서 사용)

Softmax 함수는 전체 벡터에 의존하는 함수이므로 **벡터의 도함수**는 야코비안 행렬로 표현됩니다.

단일 클래스의 경우 (크로스 엔트로피와 함께 쓰일 때):

\[
\frac{\partial L}{\partial z_i} = p_i - y_i
\]

- \( y_i \): 정답 one-hot 벡터
- \( p_i \): softmax 출력값

→ Cross-Entropy Loss와 함께 쓰면 미분이 간단해집니다!

---

## 📌 비교: Sigmoid vs Softmax

| 항목         | Sigmoid                     | Softmax                         |
|--------------|-----------------------------|----------------------------------|
| 사용 목적    | 이진 분류 (2-class)         | 다중 클래스 분류 (Multi-class) |
| 출력 범위    | (0, 1)                      | (0, 1), 전체 합 = 1              |
| 출력 해석    | 단일 확률                   | 확률 분포 (클래스별 확률)       |

---

## 📌 주요 활용처

- 다중 클래스 분류 문제의 출력층 (ex: ImageNet, 문장 분류 등)
- Attention에서 가중치 정규화 (Transformer 등)

---

