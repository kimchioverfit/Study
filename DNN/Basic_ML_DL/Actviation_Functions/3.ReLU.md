
# ReLU 함수 (Rectified Linear Unit)

## 📌 정의 (수학식)

ReLU 함수는 입력값이 0보다 크면 그대로 출력하고, 0 이하이면 0을 출력하는 간단한 비선형 함수입니다.

\[
f(x) = \max(0, x)
\]

---

## 📌 그래프

```
    |
  y |        /
    |       /
    |      /
    |     /
    |----●----------> x
    |    
    |
```

---

## 📌 미분

ReLU 함수의 미분은 다음과 같습니다:

\[
f'(x) =
\begin{cases}
1 & \text{if } x > 0 \\
0 & \text{if } x \leq 0
\end{cases}
\]

---

## 📌 장점

- ✅ 계산이 간단하고 빠름
- ✅ Gradient Vanishing 문제 완화
- ✅ Sparse activation (많은 뉴런이 0이 되어 효율적)

---

## 📌 단점

- ❌ Dying ReLU 문제: 뉴런이 0만 출력하게 되는 경우가 있음
- ❌ 출력값이 음수로 가지 않기 때문에 중심이 0이 아님

---

## 📌 활용

- 대부분의 CNN, MLP, Transformer 모델의 기본 활성화 함수로 사용됨
