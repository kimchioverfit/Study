
# 📌 딥러닝에서의 차원 축소 (Dimensionality Reduction)

---

## ✅ 개요

차원 축소는 고차원의 데이터를 보다 적은 수의 특성(feature)으로 변환하는 기법입니다.  
딥러닝에서는 **입력 데이터의 복잡도 감소**, **과적합 방지**, **시각화** 등의 목적으로 자주 사용됩니다.

---

## ✅ 왜 필요한가?

- **고차원의 저주 (Curse of Dimensionality)**  
  특성의 수가 증가할수록 학습 데이터가 희소해지고, 모델 학습이 어려워집니다.

- **계산 자원 절감**  
  차원을 줄이면 연산량이 감소하여 메모리와 학습 시간이 줄어듭니다.

- **노이즈 제거**  
  중요하지 않은 정보를 제거해 더 일반화된 모델을 만들 수 있습니다.

- **시각화 용도**  
  2D나 3D로 축소하면 데이터를 시각적으로 이해할 수 있습니다.

---

## ✅ 주요 기법

### 1. 선형 기법

- **PCA (Principal Component Analysis)**  
  공분산을 기준으로 데이터를 분산이 큰 방향으로 투영하여 축소하는 기법.

- **LDA (Linear Discriminant Analysis)**  
  클래스 간 분산을 최대화하고, 클래스 내 분산을 최소화하여 축소.

### 2. 비선형 기법

- **t-SNE (t-distributed Stochastic Neighbor Embedding)**  
  고차원의 유사도를 저차원에서 보존하면서 시각화에 적합한 방식으로 축소.

- **UMAP (Uniform Manifold Approximation and Projection)**  
  t-SNE보다 빠르고 구조 보존력이 높은 최신 차원 축소 기법.

---

## ✅ 딥러닝과의 연관

- **오토인코더 (Autoencoder)**  
  인코더(encoder)에서 입력을 저차원 잠재 벡터(latent vector)로 축소함.  
  차원 축소뿐만 아니라 이상치 탐지, 생성 모델 등에 사용됨.

- **Preprocessing 단계에서 활용**  
  PCA 등을 통해 데이터 전처리 단계에서 차원 축소를 적용한 뒤 딥러닝 모델에 입력.

---

## ✅ 주의점

- 정보 손실이 발생할 수 있음  
- 비선형 기법은 계산 비용이 크거나 하이퍼파라미터 튜닝이 까다로울 수 있음  
- 해석력이 낮아질 수 있음 (특히 Autoencoder 등 비지도학습 기반)
